{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e22b0d8-61b2-419a-a106-179c6fabfb8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Programación Big Data.\n",
    "\n",
    "## Autor: Victor Simo Lozano\n",
    "\n",
    "## Actividad 8\n",
    "<p>Spark MLib.</p>\n",
    "    \n",
    "\n",
    "<hr style=\"border-color:red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07a5c3-d17f-46d8-88e0-39fc0d81be93",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbf348-aff7-4e64-b2a8-cd0fdd819cec",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a63903-91e0-44e3-8b3b-3e4bdc42a445",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px; text-align:justify\"><b>PRIMERA PARTE.-</b><br></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac27de-a2ea-4d60-bdc2-075e159cec02",
   "metadata": {},
   "source": [
    "Realizar predicción de Iris Dataset usando **Spark MLib**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff7ace",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b27ea",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px; text-align:justify\"><b>1: </b>Importación de librería.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc88415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, OneHotEncoderModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957af41",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px; text-align:justify\"><b>2: </b>Crear la sesión Spark y lectura del set de datos.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c2bbc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/14 08:10:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Nombre de la sesión 'irisDataset'\n",
    "spark = SparkSession.builder.appName('irisDataset').getOrCreate()\n",
    "# Cargamos el set de datos del archivo csv\n",
    "df = spark.read.csv('iris.csv', header = True).cache()\n",
    "# Mostrar dataframe\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26313c",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px; text-align:justify\"><b>3: </b>Análisis del set de datos.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe7db7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de datos del dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2db85b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esquema de los datos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9546997",
   "metadata": {},
   "source": [
    "Apreciamos que para las cuatro primeras columnas, como tipo de dato tenemos **\"string\"** y observamos que debería ser **\"float\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9165f1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|\n",
      "+------------+-----------+------------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|\n",
      "|         4.9|        3.0|         1.4|        0.2|\n",
      "|         4.7|        3.2|         1.3|        0.2|\n",
      "|         4.6|        3.1|         1.5|        0.2|\n",
      "|         5.0|        3.6|         1.4|        0.2|\n",
      "+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sepal_length', 'float'),\n",
       " ('sepal_width', 'float'),\n",
       " ('petal_length', 'float'),\n",
       " ('petal_width', 'float')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casting de los datos string a float mediante estructura sql\n",
    "df_casted = df.select(sql.col('sepal_length').cast('float'),\n",
    "                    sql.col('sepal_width').cast('float'),\n",
    "                    sql.col('petal_length').cast('float'),\n",
    "                    sql.col('petal_width').cast('float')\n",
    "                   )\n",
    "\n",
    "df_casted.show(5)\n",
    "df_casted.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed4e80",
   "metadata": {},
   "source": [
    "**Nota:** Se puede observar que los datos se ha convertido a *float* como se ha programado. Pero el dataframe solo tiene las columnas escogidas en sentencia *sql*. Es decir, se ha de escoger todas las columnas que se desea en el nuevo dataframe y aplicar cast a aquellas que se vea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483e9bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sepal_length', 'float'),\n",
       " ('sepal_width', 'float'),\n",
       " ('petal_length', 'float'),\n",
       " ('petal_width', 'float'),\n",
       " ('species', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.select(sql.col('sepal_length').cast('float'),\n",
    "               sql.col('sepal_width').cast('float'),\n",
    "               sql.col('petal_length').cast('float'),\n",
    "               sql.col('petal_width').cast('float'),\n",
    "               sql.col('species')\n",
    "              )\n",
    "\n",
    "df.show(5)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762ee1f",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px; text-align:justify\"><b>4: </b>Cribado de valores.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e16a6f",
   "metadata": {},
   "source": [
    "Busqueda de valores nulos para las columnas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3063679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|           0|          0|           0|          0|      0|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([sql.count(sql.when(sql.isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f72c53",
   "metadata": {},
   "source": [
    "Como resultado se ve que es un dataset con todos los datos válidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f1c36",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px; text-align:justify\"><b>5: </b>Feature Engineering.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699837bc",
   "metadata": {},
   "source": [
    "En el ML, existe una serie de procesos que se va siguiendo hasta llegar al último estado de nuestros estudios. Estos pasos se va siguiendo de forma estructurada, uno tras otro.<br>\n",
    "El concepto de **Pipeline** viene a trabajar con esta secuencia de pasos en nuestro dataset, cuya metodología de trabajo es aplicar los cambios a nuestro dataframe hasta llegar a ese último estado que deseamos.\n",
    "\n",
    "**Fuente:** <a href=\"https://medium.com/@nutanbhogendrasharma/role-of-stringindexer-and-pipelines-in-pyspark-ml-feature-b79085bb8a6c\">Role of stringindexer and pipelines in pyspark ml feature</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06f741",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7e471",
   "metadata": {},
   "source": [
    "Trabajar con los valores indice para las *species*. Mediante el uso de ***StringIndexer*** se añade un índice numérico para cada entrada de especie que haya en la columna de specie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65d196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['virginica', 'versicolor', 'setosa']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores de especie para el dataframe\n",
    "[i.species for i in df.select('species').distinct().collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5839a855",
   "metadata": {},
   "source": [
    "Tenemos tres tipos de especie, por tanto, obtendremos tres indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7546e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar indexer\n",
    "species_indexer = StringIndexer(inputCol='species', outputCol='species_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d033b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================================>          (61 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|   species|species_index|\n",
      "+----------+-------------+\n",
      "|    setosa|          0.0|\n",
      "| virginica|          2.0|\n",
      "|versicolor|          1.0|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Seleccionar las columnas de especie y eliminar valores repetidos.\n",
    "df_sample = species_indexer.fit(df).transform(df)\n",
    "df_sample = df_sample.select(sql.col('species'),\n",
    "                               sql.col('species_index')\n",
    "                              )\n",
    "\n",
    "df_sample.dropDuplicates().show()\n",
    "\n",
    "# df_species.dropDuplicates(['species','species_indexer']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184281e4",
   "metadata": {},
   "source": [
    "Como se ve, se ha creado tantos indices como valroes de especie únicos hay. De este modo, todos los valores de *setosa* tendrán un index 0.0, *virginica* de 2.0 y *versicolor* de 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684dd030",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db7018",
   "metadata": {},
   "source": [
    "***OneHotEncoder*** permite generar un vector binario a partir de datos categoricos. <br>\n",
    "\n",
    "Como es el caso de *irisDataset*, tenemos tres tipos de especies, por lo que para poder realizar algunas predicciones, necesitamos poder evaluarlo en base a datos binarios. Por tanto, requerimos de este paso. <br>\n",
    "\n",
    "Como paso previo, se necesita hacer uso de *StringIndexer* ya que no se puede aplicar a columnas de string *OneHotEncoder*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee224b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar vectorizado binario\n",
    "species_vectorize = OneHotEncoder(inputCol=species_indexer.getOutputCol(), outputCol='species_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71242c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba para obtener columnas separadas de species y despues juntarlas en una con VectorAssembler\n",
    "# Convertir a pandas\n",
    "# df_pandas = df.toPandas()\n",
    "# df_pandas = pd.get_dummies(df_pandas, drop_first=False)\n",
    "# display(df_pandas.head())\n",
    "\n",
    "# # Convertir a spark\n",
    "# df_spark = spark.createDataFrame(df_pandas)\n",
    "# df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46348a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c073c2a",
   "metadata": {},
   "source": [
    "***VectorAssembler*** es una funcion que nos permite agrupar en un único vector características de una serie de columnas de nuestro set de datos, muy útil para entrenar modelos de ML de *logistic regresion* o *decision trees*.<br>\n",
    "\n",
    "Dicho lo cual, ahora se va a combinar los valores numéricos para las columnas de anchura y largura de petalo y sepalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df15049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas a agrupar en vector\n",
    "input_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "# Aplicar vectorizado\n",
    "values_vectorize = VectorAssembler(inputCols=input_cols, outputCol='values_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f4909",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484d867",
   "metadata": {},
   "source": [
    "Finalmente, crear nuestro modelo haciendo uso de **Pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51ca0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-------------+--------------+--------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|species_index|species_vector|       values_vector|\n",
      "+------------+-----------+------------+-----------+-------+-------------+--------------+--------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|          0.0| (2,[0],[1.0])|[5.09999990463256...|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|          0.0| (2,[0],[1.0])|[4.90000009536743...|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|          0.0| (2,[0],[1.0])|[4.69999980926513...|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|          0.0| (2,[0],[1.0])|[4.59999990463256...|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|          0.0| (2,[0],[1.0])|[5.0,3.5999999046...|\n",
      "+------------+-----------+------------+-----------+-------+-------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear el dataframe con los Feature Engineering desarrollados\n",
    "pipeline = Pipeline(stages=[species_indexer, species_vectorize, values_vectorize])\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5558115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+-------------+--------------+--------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   species|species_index|species_vector|       values_vector|\n",
      "+------------+-----------+------------+-----------+----------+-------------+--------------+--------------------+\n",
      "|         5.6|        3.0|         4.1|        1.3|versicolor|          1.0| (2,[1],[1.0])|[5.59999990463256...|\n",
      "|         5.7|        2.6|         3.5|        1.0|versicolor|          1.0| (2,[1],[1.0])|[5.69999980926513...|\n",
      "|         6.5|        3.2|         5.1|        2.0| virginica|          2.0|     (2,[],[])|[6.5,3.2000000476...|\n",
      "|         4.6|        3.6|         1.0|        0.2|    setosa|          0.0| (2,[0],[1.0])|[4.59999990463256...|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|          0.0| (2,[0],[1.0])|[4.90000009536743...|\n",
      "+------------+-----------+------------+-----------+----------+-------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Desordenar los valores ya que vienen ordenados en el fichero .csv\n",
    "df_transformed = df_transformed.orderBy(sql.rand())\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f93912e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_length: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- species_index: double (nullable = false)\n",
      " |-- species_vector: vector (nullable = true)\n",
      " |-- values_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258943a",
   "metadata": {},
   "source": [
    "***randomSplit*** nos permite separar el set de datos en un conjunto para entrenar nuestro modelo y un segundo para realizar la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dde8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar el set de datos en datos de entrenamiento y test\n",
    "(train_df, test_df) = df_transformed.randomSplit([0.80,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a83ae",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4748c9",
   "metadata": {},
   "source": [
    "***Decision Tree Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebf5a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(labelCol='species_index', featuresCol='values_vector', maxDepth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0374aa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_length: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- species_index: double (nullable = false)\n",
      " |-- species_vector: vector (nullable = true)\n",
      " |-- values_vector: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+------------+-----------+------------+-----------+----------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   species|prediction|\n",
      "+------------+-----------+------------+-----------+----------+----------+\n",
      "|         4.0|        0.9|        10.5|        0.4|    setosa|       2.0|\n",
      "|         4.4|        3.2|         1.3|        0.2|    setosa|       0.0|\n",
      "|         4.5|        2.3|         1.3|        0.3|    setosa|       0.0|\n",
      "|         4.7|        3.2|         1.6|        0.2|    setosa|       0.0|\n",
      "|         4.9|        2.5|         4.5|        1.7| virginica|       1.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|    setosa|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|       0.0|\n",
      "|         5.1|        3.5|         1.4|        0.3|    setosa|       0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2|    setosa|       0.0|\n",
      "|         5.5|        2.6|         4.4|        1.2|versicolor|       1.0|\n",
      "|         5.5|        3.5|         1.3|        0.2|    setosa|       0.0|\n",
      "|         5.6|        2.5|         3.9|        1.1|versicolor|       1.0|\n",
      "|         5.8|        2.6|         4.0|        1.2|versicolor|       1.0|\n",
      "|         6.0|        2.2|         4.0|        1.0|versicolor|       1.0|\n",
      "|         6.2|        2.2|         4.5|        1.5|versicolor|       1.0|\n",
      "|         6.2|        2.8|         4.8|        1.8| virginica|       2.0|\n",
      "|         6.2|        2.9|         4.3|        1.3|versicolor|       1.0|\n",
      "|         6.3|        3.4|         5.6|        2.4| virginica|       2.0|\n",
      "|         6.4|        2.7|         5.3|        1.9| virginica|       2.0|\n",
      "+------------+-----------+------------+-----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Precisión Decission Tree Classifier = '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9032258064516129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[DTC])\n",
    "model_DTC = pipeline.fit(train_df)\n",
    "\n",
    "# Prediccion\n",
    "pred_DTC= model_DTC.transform(test_df)\n",
    "pred_DTC.printSchema()\n",
    "pred_DTC.select(sql.col('sepal_length'),sql.col('sepal_width'),\n",
    "               sql.col('petal_length'),sql.col('petal_width'),\n",
    "               sql.col('species'),sql.col('prediction')).show()\n",
    "\n",
    "# Precision\n",
    "evaluator_DTC = MulticlassClassificationEvaluator(labelCol='species_index', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy_DTC = evaluator_DTC.evaluate(pred_DTC)\n",
    "display('Precisión Decission Tree Classifier = ', accuracy_DTC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e157517",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6172dc",
   "metadata": {},
   "source": [
    "***Gradient-boosted tree classiffier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa736812",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC = GBTClassifier(labelCol='species_vector', featuresCol='values_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee5169a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column species_vector must be of type numeric but was actually of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2440/194863451.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGBC\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_GBC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Prediccion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_GBC\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel_DTC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column species_vector must be of type numeric but was actually of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>>."
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[GBC])\n",
    "model_GBC = pipeline.fit(train_df)\n",
    "\n",
    "# Prediccion\n",
    "pred_GBC= model_DTC.transform(test_df)\n",
    "pred_GBC.printSchema()\n",
    "pred_GBC.select(sql.col('sepal_length'),sql.col('sepal_width'),\n",
    "               sql.col('petal_length'),sql.col('petal_width'),\n",
    "               sql.col('species'),sql.col('prediction')).show()\n",
    "\n",
    "# Precision\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='species_vector', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(pred_GBC)\n",
    "display('Precisión Decission Tree Classifier = ', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05bdd50",
   "metadata": {},
   "source": [
    "Para la ejecución de ***Gradient-boosted tree classiffier*** se obtiene un error en el formato de la columna de label. Este algoritmo solo permite predicciones binarias, por lo que para conseguirlo se ha realizado **OneHotEncoder** como se ha explicado, pero el formato de columna no es aceptado. <br>\n",
    "\n",
    "Visto esto, se ha probado a realizar una segregación de la columa *species* con **get_dummies** pasando por pandas y más adelante mediante **VectorAssembler** obtener una columa. Tras realizar el nuevo pipeline, a la hora de ejecutar el algoritmo, se obtiene el mismo error de tipo de columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53f4ed",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087f033",
   "metadata": {},
   "source": [
    "***Random Forest Classiffier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c6f9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(labelCol='species_index', featuresCol='values_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66189e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   species|prediction|\n",
      "+------------+-----------+------------+-----------+----------+----------+\n",
      "|         4.0|        0.9|        10.5|        0.4|    setosa|       2.0|\n",
      "|         4.4|        3.2|         1.3|        0.2|    setosa|       0.0|\n",
      "|         4.5|        2.3|         1.3|        0.3|    setosa|       0.0|\n",
      "|         4.7|        3.2|         1.6|        0.2|    setosa|       0.0|\n",
      "|         4.9|        2.5|         4.5|        1.7| virginica|       1.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|    setosa|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|       0.0|\n",
      "|         5.1|        3.5|         1.4|        0.3|    setosa|       0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2|    setosa|       0.0|\n",
      "|         5.5|        2.6|         4.4|        1.2|versicolor|       1.0|\n",
      "|         5.5|        3.5|         1.3|        0.2|    setosa|       0.0|\n",
      "|         5.6|        2.5|         3.9|        1.1|versicolor|       1.0|\n",
      "|         5.8|        2.6|         4.0|        1.2|versicolor|       1.0|\n",
      "|         6.0|        2.2|         4.0|        1.0|versicolor|       1.0|\n",
      "|         6.2|        2.2|         4.5|        1.5|versicolor|       1.0|\n",
      "|         6.2|        2.8|         4.8|        1.8| virginica|       2.0|\n",
      "|         6.2|        2.9|         4.3|        1.3|versicolor|       1.0|\n",
      "|         6.3|        3.4|         5.6|        2.4| virginica|       2.0|\n",
      "|         6.4|        2.7|         5.3|        1.9| virginica|       2.0|\n",
      "+------------+-----------+------------+-----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Precisión Random Forest Classifier = '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9032258064516129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[RFC])\n",
    "model_RFC = pipeline.fit(train_df)\n",
    "\n",
    "# Prediccion\n",
    "pred_RFC= model_DTC.transform(test_df)\n",
    "# pred_RFC.printSchema()\n",
    "pred_RFC.select(sql.col('sepal_length'),sql.col('sepal_width'),\n",
    "               sql.col('petal_length'),sql.col('petal_width'),\n",
    "               sql.col('species'),sql.col('prediction')).show()\n",
    "\n",
    "# Precision\n",
    "evaluator_RFC = MulticlassClassificationEvaluator(labelCol='species_index', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy_RFC= evaluator_RFC.evaluate(pred_RFC)\n",
    "display('Precisión Random Forest Classifier = ', accuracy_RFC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
